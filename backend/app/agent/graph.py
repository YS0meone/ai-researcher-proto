import uuid
from langchain_core.tools import tool
from app.db.schema import S2Paper
from app.agent.paper_finder import paper_finder
from app.agent.paper_finder_fast import paper_finder_fast_graph
from app.agent.qa import qa_graph
from typing import Tuple
from langgraph.graph import StateGraph
from app.agent.states import State
import logging
from app.core.config import settings
from app.core.logging_config import setup_logging
from langchain.chat_models import init_chat_model
from langgraph.graph import START, END
from app.agent.utils import setup_langsmith
from pydantic import BaseModel, Field
from langchain.messages import SystemMessage, AIMessage, ToolMessage, HumanMessage
from app.tools.search import get_paper_details
from langgraph.types import Command
from langchain.tools import ToolRuntime
from langgraph.graph.ui import push_ui_message
from langgraph.prebuilt import ToolNode, tools_condition
from typing import Literal, List
from app.agent.utils import get_paper_info_text

# Initialize logging
setup_logging()

setup_langsmith()
logger = logging.getLogger(__name__)

@tool
def find_papers(runtime: ToolRuntime) -> Command:
    """
    Find papers using the optimized query generated by the previous workflow.
    It would update the current papers list with the new papers found.
    Trust the result from the tools would find the most relevant papers.
    """
    
    user_query = runtime.state.get("optimized_query", "")
    papers = runtime.state.get("papers", [])

    logger.info(f"Finding papers for query: {user_query[:100]}{'...' if len(user_query) > 100 else ''}")
    logger.debug(f"Current paper count: {len(papers)}")

    state = {"optimized_query": user_query, "papers": papers, "messages": [HumanMessage(content=user_query)]}
    result = paper_finder_fast_graph.invoke(state)
    papers = result["papers"]

    logger.info(f"Found {len(papers)} papers")

    # Return papers via Command - UI message will be pushed from agent node
    return Command(
        update={"papers": result["papers"],
        "messages": [ToolMessage(content=f"I found {len(result['papers'])} papers for your query.", tool_call_id=runtime.tool_call_id)]
        })

@tool
def answer_question(runtime: ToolRuntime) -> str:
    """
    Answer the user question based on the current papers.
    If user has selected specific papers, focus the analysis on those papers.
    Trust the result from the tools would answer the user question based on the papers and forward the answer to the user.
    """
    user_query = runtime.state.get("optimized_query", "")
    papers = runtime.state.get("papers", [])
    selected_ids = runtime.state.get("selected_paper_ids", [])

    logger.info(f"Answering question for query: {user_query[:100]}{'...' if len(user_query) > 100 else ''}")
    logger.debug(f"Selected paper count: {len(selected_ids)}, Total papers: {len(papers)}")

    if not selected_ids:
        logger.warning("No papers selected for QA")
        return "No papers selected for QA. Please select papers first."

    # If user selected specific papers, use those for QA
    qa_state = {
        "messages": [HumanMessage(content=user_query)],
        "papers": papers,
        "selected_paper_ids": selected_ids,
        "user_query": user_query,
    }

    logger.debug("Invoking QA graph")
    result = qa_graph.invoke(qa_state)
    logger.info("QA graph completed successfully")
    return result["final_answer"]

def query_clarification(state: State):
    logger.debug("Query clarification node invoked")
    step_message = AIMessage(id=str(uuid.uuid4()), content="")
    test_ui_message = push_ui_message(
        "steps",
        {
            "steps": [
                {
                    "id": "find_papers",
                    "label": "Finding papers",
                    "status": "running",
                    "description": f"testing",
                },
                {
                    "id": "find_papers",
                    "label": "Finding papers",
                    "status": "running",
                    "description": f"testing",
                },{
                    "id": "find_papers",
                    "label": "Finding papers",
                    "status": "running",
                    "description": f"testing",
                },
                {
                    "id": "find_papers",
                    "label": "Finding papers",
                    "status": "running",
                    "description": f"testing",
                }
            ],
            "currentStep": "find_papers",
        },
        message=step_message
    )
    logger.info(f"[query_clarification] Pushed step tracking UI message: {test_ui_message['id']}")
    system_prompt = f"""
    You are an expert in clarifying user queries for a research assistant.
    You need to decide if the user's query is clear or it needs clarification.
    Take the previous messages into account if there is any.
    Make the decision and provide your reasoning for the decision.
    """

    class QueryIsClear(BaseModel):
        reasoning: str = Field(description="The reasoning why the user's query is clear")

    class QueryNeedsClarification(BaseModel):
        reasoning: str = Field(description="The reasoning why the user's query needs clarification")
        clarification: str = Field(description="The clarification for the user's query")

    tools = [QueryIsClear, QueryNeedsClarification]
    structured_model = supervisor_model.bind_tools(tools, tool_choice="any")
    msg = structured_model.invoke([
        SystemMessage(content=system_prompt)
    ] + state["messages"])

    response = msg.tool_calls[0]["args"]

    if "clarification" not in response:
        logger.info("Query is clear, proceeding to optimization")
        return {"is_clear": True, "messages": [step_message]}
    else:
        logger.info("Query needs clarification, requesting user input")
        logger.debug(f"Clarification requested: {response['clarification'][:100]}...")
        return {
            "messages": [step_message, AIMessage(content=response["clarification"])],
            "is_clear": False
        }

def query_optimization(state: State):
    logger.debug("Query optimization node invoked")

    system_prompt = f"""
    You are an expert in optimizing user queries for a search agent for academic papers.
    Your goals is to rephrase the user query to be more specific and to be more likely to help the subagent find the most relevant papers and answer the user's question.
    There might be some clarification happened before this node, you should take that into account.
    If the user's query is good enough, you may repeat the user's query as the optimized query or change it slightly to be more specific.
    If the user's query is not good enough, you should optimize it.
    The optimized query should be self-contained and should not require any additional context.
    """

    class QueryOptimizationOutput(BaseModel):

        reasoning: str = Field(description="The reasoning for your optimization")
        optimized_query: str = Field(description="The optimized query for the user's query")

    tools = [QueryOptimizationOutput]
    structured_model = supervisor_model.bind_tools(tools, tool_choice="QueryOptimizationOutput")
    msg = structured_model.invoke([
        SystemMessage(content=system_prompt)
    ] + state["messages"])
    response = msg.tool_calls[0]["args"]

    logger.info(f"Query optimized: {response['optimized_query'][:100]}{'...' if len(response['optimized_query']) > 100 else ''}")
    logger.debug(f"Optimization reasoning: {response['reasoning'][:150]}...")

    message = f"The optimized query is: {response['optimized_query']}\n\nReasoning: {response['reasoning']}"

    return {"messages": [AIMessage(content=message)], "optimized_query": response['optimized_query']}

def should_clarify(state: State):
    is_clear = state.get("is_clear", True)
    route = "optimize" if is_clear else "end"
    return route

supervisor_model = init_chat_model(model=settings.AGENT_MODEL_NAME, api_key=settings.GEMINI_API_KEY)
tools = [find_papers, get_paper_details]

supervisor_prompt = """
You are a supervisor for a research assistant system.
You are provided with two tools: find_papers, and get_paper_details.
The find_papers tool is used to find papers related to the user query.
The get_paper_details tool is used to get the details of a paper currently in the paper list.
If you don't have enough papers, you should call the find_papers tool.
And then you should call the get_paper_details tool to get the details of the papers.
You can only call the find papers tool once.
"""

# Bind tools to the supervisor model
supervisor_model_with_tools = supervisor_model.bind_tools(tools)

@tool
def generate_plan(plan_choice: Literal["find_then_qa", "find_only", "qa_only"]) -> str:
    """
    Generate a plan for the research assistant to follow.
    Args: 
    - plan_choice: The plan you are going to follow.
        - "qa_only": Choose this plan if you think the current paper list is enough to answer the user's question
        - "find_then_qa": Choose this plan if user have a specific question about some papers that is not present in the current paper list
        - "find_only": Choose this plan if the user is interested in finding certain papers and does not ask any specific question about them
    Returns:
    - plan: The plan you are going to follow.
    """
    return ""
    
def supervisor_agent_node(state: State):
    """
    Supervisor agent node that:
    1. Checks if last message is a ToolMessage from find_papers
    2. If so, extracts papers and calls push_ui_message
    3. Otherwise, invokes the model to decide next action
    """
    logger.debug("Supervisor agent node invoked")

    messages = state["messages"]
    last_message = messages[-1] if messages else None

    # ========================================
    # POST-PROCESSING: Handle tool results
    # ========================================
    if isinstance(last_message, ToolMessage):
        logger.info(f"Post-processing ToolMessage from: {last_message.name}")

        if last_message.name == "find_papers":
            papers = state.get("papers", [])
            logger.debug(f"Retrieved {len(papers)} papers from state")

            if papers and "found" in last_message.content.lower() and "papers" in last_message.content.lower():
                logger.info(f"Pushing UI message with {len(papers)} papers")

                try:
                    # Convert S2Paper objects to dicts if needed
                    papers_data = [p.model_dump() if hasattr(p, 'model_dump') else p for p in papers]
                    logger.debug(f"Converted {len(papers_data)} papers to dict format")

                    # Create AI message
                    ai_message = AIMessage(
                        id=str(uuid.uuid4()),
                        content=f"I found {len(papers)} papers for your query.",
                    )

                    # Push UI message FROM NODE (officially supported!)
                    ui_msg = push_ui_message(
                        "papers",
                        {"papers": papers_data},
                        message=ai_message
                    )

                    logger.info(f"UI message pushed successfully: {ui_msg['id']}")

                    # Return AI message to state and continue to next plan step
                    return {"messages": [ai_message]}

                except Exception as e:
                    logger.error(f"Failed to push UI message: {e}", exc_info=True)
                    # Continue to next plan step even if UI push fails
                    return {}
            else:
                # No papers or unexpected message format
                logger.warning("find_papers returned but no papers found or unexpected message format")
                return {}

        elif last_message.name == "answer_question":
            logger.info("answer_question completed successfully")
            return {"messages": [AIMessage(content=last_message.content)]}

        else:
            logger.warning(f"Unknown tool call: {last_message.name}")
            return {}
    
    # ========================================
    # PLAN GENERATION & EXECUTION
    # ========================================
    plan = state.get("plan_steps", [])

    # Generate plan if it doesn't exist
    if len(plan) == 0:
        logger.info("Generating new plan")

        # Check if user has selected specific papers
        selected_ids = state.get("selected_paper_ids", [])
        selected_context = ""
        if selected_ids:
            logger.info(f"User has selected {len(selected_ids)} specific papers for focused analysis")
            selected_context = f"\n\nIMPORTANT: User has selected {len(selected_ids)} specific papers for focused analysis (IDs: {', '.join(selected_ids[:3])}{'...' if len(selected_ids) > 3 else ''}). Consider these papers when planning."

        plan_prompt = f"""
        You are a supervisor for a research assistant system.
        Generate a plan using the provided tool with provided context.{selected_context}
        """
        planner = supervisor_model_with_tools.bind_tools([generate_plan], tool_choice="generate_plan")
        planner_context = f"""
        User query: {state.get("optimized_query", "")}
        Current papers: {get_paper_info_text(state.get("papers", []))}
        """
        logger.debug("Invoking planner model")
        response = planner.invoke([
            SystemMessage(content=plan_prompt),
            HumanMessage(content=planner_context)
        ])
        plan_choice = response.tool_calls[0]["args"]["plan_choice"]
        logger.info(f"Planner chose: {plan_choice}")

        if plan_choice == "qa_only":
            plan = ["answer_question", "end"]
        elif plan_choice == "find_then_qa":
            plan = ["find_papers", "answer_question", "end"]
        elif plan_choice == "find_only":
            plan = ["find_papers", "end"]
        else:
            logger.error(f"Invalid plan choice: {plan_choice}")
            raise ValueError(f"Invalid plan choice: {plan_choice}")

        logger.info(f"Generated plan: {plan}")

    # Execute next step in plan
    if not plan:
        logger.warning("Plan is empty, ending execution")
        return {}

    current_step = plan.pop(0)
    logger.info(f"Executing plan step: {current_step}")

    if current_step == "end":
        logger.info("Plan completed successfully")
        return {}
    
    # Create manual tool call for the next step
    manual_tool_call = {
        "name": current_step,
        "args": {},
        "id": str(uuid.uuid4())
    }

    return {"messages": [AIMessage(content="", tool_calls=[manual_tool_call])], "plan_steps": plan}

supervisor_tool_node = ToolNode(tools)


# Build the graph with decomposed supervisor
graph = StateGraph(State)
graph.add_node("query_clarification", query_clarification)
graph.add_node("query_optimization", query_optimization)
graph.add_node("supervisor_agent", supervisor_agent_node)
graph.add_node("supervisor_tools", supervisor_tool_node)

graph.add_edge(START, "query_clarification")
graph.add_conditional_edges("query_clarification", should_clarify, {"optimize": "query_optimization", "end": END})
graph.add_edge("query_optimization", "supervisor_agent")

# Add conditional edges for the supervisor agent loop
graph.add_conditional_edges(
    "supervisor_agent",
    tools_condition,
    {
        "tools": "supervisor_tools",
        "__end__": END
    }
)
graph.add_edge("supervisor_tools", "supervisor_agent")

graph = graph.compile().with_config({"recursion_limit": 100})