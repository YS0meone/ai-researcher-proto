import uuid
from langchain_core.tools import tool
from app.agent.paper_finder_fast import paper_finder_fast_graph
from app.agent.qa import qa_graph
from langgraph.graph import StateGraph
from app.agent.states import SupervisorState
import logging
from app.core.config import settings
from app.core.logging_config import setup_logging
from langchain.chat_models import init_chat_model
from langgraph.graph import START, END
from pydantic import BaseModel, Field
from langchain.messages import SystemMessage, AIMessage, ToolMessage, HumanMessage
from app.tools.search import get_paper_details
from langgraph.types import Command, interrupt
from langchain.tools import ToolRuntime
from langgraph.graph.ui import push_ui_message
from langgraph.prebuilt import ToolNode, tools_condition
from typing import List, Literal
from app.agent.ui_manager import UIManager
from app.core.schema import StepName, StepStatus
from app.agent.utils import get_paper_abstract

DO_NOT_RENDER_ID_PREFIX = "do-not-render-"

setup_logging()

logger = logging.getLogger(__name__)


def _optimize_for_search(messages: list) -> str:
    """Derive a keyword-style search query from the full conversation history."""
    class SearchQueryOutput(BaseModel):
        search_query: str = Field(
            description=(
                "A keyword-style query for semantic academic paper retrieval. "
                "5-15 words. Include core concepts, method names, domain terms, "
                "and any paper/author names mentioned in the conversation. "
                "Do NOT write a full sentence — use noun phrases and keywords only. "
                "Example: 'transformer self-attention mechanism NLP Vaswani 2017'"
            )
        )

    system = SystemMessage(content=(
        "You are an expert at generating academic paper search queries. "
        "Given the conversation history below, produce a keyword-style search query "
        "that captures what the user wants to find. "
        "Resolve all references (e.g. 'that paper', 'it', 'their method') using context from earlier messages."
    ))
    model = init_chat_model(model=settings.AGENT_MODEL_NAME, api_key=settings.GEMINI_API_KEY)
    structured = model.bind_tools([SearchQueryOutput], tool_choice="SearchQueryOutput")
    msg = structured.invoke([system] + messages)
    return msg.tool_calls[0]["args"]["search_query"]


def _optimize_for_qa(messages: list) -> str:
    """Derive a fully self-contained research question from the full conversation history."""
    class QAQueryOutput(BaseModel):
        qa_query: str = Field(
            description=(
                "A fully self-contained research question. "
                "All pronouns and vague references must be replaced with explicit names "
                "using context from earlier messages. "
                "The question must make complete sense without any prior context."
            )
        )

    system = SystemMessage(content=(
        "You are an expert at reformulating research questions. "
        "Given the conversation history below, rewrite the user's latest question "
        "as a fully self-contained research question. "
        "Resolve all references (e.g. 'that paper', 'it', 'their method', 'the above') "
        "using context from earlier messages."
    ))
    model = init_chat_model(model=settings.AGENT_MODEL_NAME, api_key=settings.GEMINI_API_KEY)
    structured = model.bind_tools([QAQueryOutput], tool_choice="QAQueryOutput")
    msg = structured.invoke([system] + messages)
    return msg.tool_calls[0]["args"]["qa_query"]


@tool
def find_papers(runtime: ToolRuntime) -> Command:
    """
    Find papers using the optimized query generated by the previous workflow.
    It would update the current papers list with the new papers found.
    Trust the result from the tools would find the most relevant papers.
    """

    messages = runtime.state.get("messages", [])
    papers = runtime.state.get("papers", [])

    search_query = _optimize_for_search(messages)
    logger.info(f"Finding papers for search query: {search_query[:100]}{'...' if len(search_query) > 100 else ''}")
    logger.debug(f"Current paper count: {len(papers)}")

    state = {"optimized_query": search_query, "papers": papers, "messages": [HumanMessage(content=search_query)]}
    result = paper_finder_fast_graph.invoke(state)
    papers = result["papers"]

    logger.info(f"Found {len(papers)} papers")

    return Command(
        update={"papers": result["papers"],
        "messages": [ToolMessage(content=f"I found {len(result['papers'])} papers for your query.", tool_call_id=runtime.tool_call_id)]
        })

@tool
def retrieve_and_answer_question(runtime: ToolRuntime) -> str:
    """
    Answer the user question based on the current papers.
    The tool would retrieve evidence from the selected papers and answer the user question based on the evidence.
    Return:
    - The answer to the user's question
    """
    messages = runtime.state.get("messages", [])
    papers = runtime.state.get("papers", [])
    selected_ids = runtime.state.get("selected_paper_ids", [])

    user_query = _optimize_for_qa(messages)
    logger.info(f"Answering question for query: {user_query[:100]}{'...' if len(user_query) > 100 else ''}")
    logger.debug(f"Selected paper count: {len(selected_ids)}, Total papers: {len(papers)}")

    if not selected_ids:
        logger.warning("No papers selected for QA")
        return "No papers selected for QA. Please select papers first."

    qa_state = {
        "messages": [HumanMessage(content=user_query)],
        "papers": papers,
        "selected_paper_ids": selected_ids,
        "user_query": user_query,
    }

    logger.debug("Invoking QA graph")
    result = qa_graph.invoke(qa_state)
    logger.info("QA graph completed successfully")
    return result["final_answer"]


supervisor_model = init_chat_model(model=settings.AGENT_MODEL_NAME, api_key=settings.GEMINI_API_KEY)
tools = [find_papers, get_paper_details, retrieve_and_answer_question]
supervisor_tool_node = ToolNode(tools)


# ─── Query clarification ──────────────────────────────────────────────────────

def query_clarification(state: SupervisorState):
    logger.debug("Query clarification node invoked")
    new_ui_tracking_message = AIMessage(id=str(uuid.uuid4()), content="")
    new_ui_tracking_id = str(uuid.uuid4())
    new_ui_state = {
        "steps": [],
        "ui_tracking_message": new_ui_tracking_message,
        "ui_tracking_id": new_ui_tracking_id
    }
    ui_manager = UIManager.from_state(new_ui_state)
    ui_manager.update_ui(StepName.QUERY_CLARIFICATION, StepStatus.RUNNING)

    system_prompt = """
    You are an expert in clarifying user queries for a research assistant.
    You need to decide if the user's query is clear or it needs clarification.
    Take the previous messages into account if there is any.
    Make the decision and provide your reasoning for the decision.
    """

    class QueryIsClear(BaseModel):
        reasoning: str = Field(description="The reasoning why the user's query is clear")

    class QueryNeedsClarification(BaseModel):
        reasoning: str = Field(description="The reasoning why the user's query needs clarification")
        clarification: str = Field(description="The clarification for the user's query")

    tools_qc = [QueryIsClear, QueryNeedsClarification]
    structured_model = supervisor_model.bind_tools(tools_qc, tool_choice="any")
    msg = structured_model.invoke([
        SystemMessage(content=system_prompt)
    ] + state["messages"])

    response = msg.tool_calls[0]["args"]

    if "clarification" not in response:
        logger.info("Query is clear, proceeding to planner")
        new_steps = ui_manager.update_ui(StepName.QUERY_CLARIFICATION, StepStatus.COMPLETED)
        return {
            "messages": [new_ui_tracking_message],
            "is_clear": True,
            "steps": new_steps,
            "ui_tracking_message": new_ui_tracking_message,
            "ui_tracking_id": new_ui_tracking_id
        }
    else:
        logger.info("Query needs clarification, requesting user input")
        new_steps = ui_manager.update_ui(StepName.QUERY_CLARIFICATION, StepStatus.COMPLETED, response["clarification"])
        return {
            "messages": [new_ui_tracking_message, AIMessage(content=response["clarification"])],
            "is_clear": False,
            "steps": new_steps,
            "ui_tracking_message": new_ui_tracking_message,
            "ui_tracking_id": new_ui_tracking_id
        }


def should_clarify(state: SupervisorState):
    is_clear = state.get("is_clear", True)
    return "planner" if is_clear else "end"


# ─── Planner ──────────────────────────────────────────────────────────────────

class PlanOutput(BaseModel):
    reasoning: str = Field(description="Why this plan was chosen")
    steps: List[Literal["find_papers", "retrieve_and_answer_question"]] = Field(
        description="Ordered list of steps to execute"
    )


def planner(state: SupervisorState):
    """Decide the sequence of steps for the current user turn."""
    ui_manager = UIManager.from_state(state)
    logger.info("Planner node invoked")
    ui_manager.update_ui(StepName.PLAN, StepStatus.RUNNING)

    papers = state.get("papers", [])
    selected_ids = state.get("selected_paper_ids", [])
    selected_paper_abstracts = ""
    if selected_ids:
        selected_paper_abstracts = get_paper_abstract(papers, selected_ids)

    plan_prompt = f"""You are a planner for a research assistant system.

The system can perform two actions:
- "find_papers": search for academic papers matching the user's query
- "retrieve_and_answer_question": retrieve evidence from selected papers and answer the user's question

Rules:
- If papers are already present AND the user is asking a question about them → ["retrieve_and_answer_question"]
- If the user only wants to find papers with no question → ["find_papers"]
- If the user wants to find papers AND ask a question → ["find_papers", "retrieve_and_answer_question"]
- If neither action fits (e.g. greeting, off-topic) → []

Current papers in context: {len(papers)}
Selected paper abstracts:
{selected_paper_abstracts if selected_paper_abstracts else "(none selected)"}
"""

    structured = supervisor_model.bind_tools([PlanOutput], tool_choice="PlanOutput")
    response = structured.invoke([
        SystemMessage(content=plan_prompt),
        *state.get("messages", []),
    ])

    plan_result = response.tool_calls[0]["args"]
    steps = plan_result.get("steps", [])
    reasoning = plan_result.get("reasoning", "")
    logger.info(f"Planner chose steps: {steps} — {reasoning}")

    # Derive a human-readable label for the UI
    if steps == ["retrieve_and_answer_question"]:
        plan_label = "qa_only"
    elif steps == ["find_papers"]:
        plan_label = "find_only"
    elif steps == ["find_papers", "retrieve_and_answer_question"]:
        plan_label = "find_then_qa"
    else:
        plan_label = "find_only"  # fallback

    new_steps = ui_manager.update_ui(StepName.PLAN, StepStatus.COMPLETED, plan_label)
    return {"plan_steps": steps, "steps": new_steps}


# ─── Executor ─────────────────────────────────────────────────────────────────

def executor(state: SupervisorState):
    """Pop the first step from the plan and issue the corresponding tool call."""
    ui_manager = UIManager.from_state(state)
    plan = list(state.get("plan_steps", []))

    if not plan:
        logger.info("Plan is empty — executor producing final end message")
        # No tool call → tools_condition will route to __end__
        return {"messages": [AIMessage(content="")], "plan_steps": []}

    current_step = plan.pop(0)
    logger.info(f"Executor dispatching step: {current_step}")

    if current_step == "find_papers":
        ui_manager.update_ui(StepName.FIND_PAPERS, StepStatus.RUNNING)
    elif current_step == "retrieve_and_answer_question":
        ui_manager.update_ui(StepName.RETRIEVE_AND_ANSWER_QUESTION, StepStatus.RUNNING)

    tool_call = {
        "name": current_step,
        "args": {},
        "id": str(uuid.uuid4()),
        "type": "tool_call",
    }
    return {
        "messages": [AIMessage(content="", tool_calls=[tool_call])],
        "plan_steps": plan,
        "steps": ui_manager.steps,
    }


# ─── Post-tool node ───────────────────────────────────────────────────────────
# Runs immediately after supervisor_tools. Commits UI messages and answer to
# state so they are checkpointed BEFORE any interrupt is issued.

def post_tool(state: SupervisorState):
    """
    Commit tool-result side-effects to state (paper list UI, final answer).
    This node always completes normally so its return value is checkpointed.
    """
    ui_manager = UIManager.from_state(state)
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None

    if not isinstance(last_message, ToolMessage):
        logger.warning("post_tool called but last message is not a ToolMessage")
        return {}

    last_tool = last_message.name
    logger.info(f"post_tool handling result of: {last_tool}")

    if last_tool == "find_papers":
        papers = state.get("papers", [])
        logger.info(f"find_papers completed — {len(papers)} papers — pushing paper list UI")

        paper_list_ui_message = AIMessage(id=str(uuid.uuid4()), content="")
        papers_data = [p.model_dump() if hasattr(p, "model_dump") else p for p in papers]
        push_ui_message(
            "papers",
            {"papers": papers_data},
            message=paper_list_ui_message,
        )
        ui_manager.update_ui(StepName.FIND_PAPERS, StepStatus.COMPLETED, len(papers))
        # paper_list_ui_message is committed to state here (node returns normally)
        return {"messages": [paper_list_ui_message], "steps": ui_manager.steps}

    elif last_tool == "retrieve_and_answer_question":
        logger.info("retrieve_and_answer_question completed — emitting final answer")
        ui_manager.update_ui(StepName.RETRIEVE_AND_ANSWER_QUESTION, StepStatus.COMPLETED)
        answer = last_message.content
        return {
            "plan_steps": [],
            "messages": [AIMessage(content=answer)],
            "steps": ui_manager.steps,
        }

    else:
        logger.warning(f"post_tool: unknown tool {last_tool}")
        return {}


def post_tool_route(state: SupervisorState) -> str:
    """Route to replanner after find_papers; go straight to END after QA."""
    messages = state.get("messages", [])
    for msg in reversed(messages):
        if isinstance(msg, ToolMessage):
            return "replanner" if msg.name == "find_papers" else "__end__"
    return "__end__"


# ─── Replanner ────────────────────────────────────────────────────────────────
# Runs only after find_papers. Interrupts so the user can select papers, then
# re-evaluates the remaining plan on resume.
# When LangGraph resumes from an interrupt it re-runs this node from the top;
# interrupt() returns the resume value immediately on the second execution.

class ReplanOutput(BaseModel):
    reasoning: str = Field(description="Why these remaining steps were chosen")
    updated_steps: List[Literal["find_papers", "retrieve_and_answer_question"]] = Field(
        description="Remaining steps to execute after this interrupt"
    )


def replanner(state: SupervisorState):
    """Interrupt for paper selection, then update the plan.

    The resume payload is a dict sent by the client:
        { "selected_paper_ids": [...], "user_message": "..." | null }

    selected_paper_ids and the user message cannot be passed as a state update
    alongside a Command(resume=...) — the LangGraph server treats `input` and
    `command` as mutually exclusive.  Embedding them in the resume value is the
    reliable way to carry them across the interrupt boundary.
    """
    ui_manager = UIManager.from_state(state)

    # Pause here — paper list UI is already in state (committed by post_tool).
    resume_payload = interrupt("select_papers")

    # ── Resumed ──────────────────────────────────────────────────────────────
    logger.info(f"Replanner resumed, payload: {resume_payload}")
    ui_manager.update_ui(StepName.REPLANNING, StepStatus.RUNNING)

    # Extract selected_paper_ids and optional user message from the resume payload.
    if isinstance(resume_payload, dict):
        selected_ids = resume_payload.get("selected_paper_ids", [])
        user_message_text = resume_payload.get("user_message") or None
    else:
        # Fallback: plain string resume (e.g. "continue")
        selected_ids = state.get("selected_paper_ids", [])
        user_message_text = str(resume_payload) if resume_payload not in (None, "continue", "") else None

    logger.info(f"Replanner: selected_ids={selected_ids}, user_message={user_message_text!r}")

    # Build the updated messages list (include the human message if provided)
    messages = list(state.get("messages", []))
    new_messages: list = []
    if user_message_text:
        new_messages.append(HumanMessage(content=user_message_text))
        messages = messages + new_messages  # used for replanner LLM context

    remaining = list(state.get("plan_steps", []))
    papers_now = state.get("papers", [])
    selected_abstracts = get_paper_abstract(papers_now, selected_ids) if selected_ids else "(none selected)"

    replan_prompt = f"""You are a replanner for a research assistant.

The user just reviewed a list of papers returned by a search.
Your job: decide which steps (if any) should still run.

Remaining steps before this interrupt: {remaining}

Rules:
- If the user's new message requests a new/different search → ["find_papers", ...] (keep QA if originally requested)
- If papers were selected and the original intent included answering a question → ["retrieve_and_answer_question"]
- If the user is satisfied or asks something unrelated → []

Current papers: {len(papers_now)}
Selected papers: {len(selected_ids)}
Selected paper abstracts:
{selected_abstracts}
"""
    structured = supervisor_model.bind_tools([ReplanOutput], tool_choice="ReplanOutput")
    response = structured.invoke([
        SystemMessage(content=replan_prompt),
        *messages,
    ])
    updated = response.tool_calls[0]["args"].get("updated_steps", [])
    logger.info(f"Replanner updated steps: {updated}")

    new_steps = ui_manager.update_ui(StepName.REPLANNING, StepStatus.COMPLETED)
    return {
        "plan_steps": updated,
        "selected_paper_ids": selected_ids,  # commit to state so QA tool can read it
        "messages": new_messages,
        "steps": new_steps,
    }


# ─── Graph assembly ───────────────────────────────────────────────────────────

graph = StateGraph(SupervisorState)
graph.add_node("query_clarification", query_clarification)
graph.add_node("planner", planner)
graph.add_node("executor", executor)
graph.add_node("supervisor_tools", supervisor_tool_node)
graph.add_node("post_tool", post_tool)
graph.add_node("replanner", replanner)

graph.add_edge(START, "query_clarification")
graph.add_conditional_edges(
    "query_clarification",
    should_clarify,
    {"planner": "planner", "end": END}
)
graph.add_edge("planner", "executor")
graph.add_conditional_edges(
    "executor",
    tools_condition,
    {"tools": "supervisor_tools", "__end__": END}
)
graph.add_edge("supervisor_tools", "post_tool")
graph.add_conditional_edges(
    "post_tool",
    post_tool_route,
    {"replanner": "replanner", "__end__": END}
)
graph.add_edge("replanner", "executor")

graph = graph.compile().with_config({"recursion_limit": 100})
