import uuid
from langchain_core.tools import tool
from app.agent.paper_finder_fast import paper_finder_fast_graph
from app.agent.qa import qa_graph
from langgraph.graph import StateGraph
from app.agent.states import SupervisorState
import logging
from app.core.config import settings
from app.core.logging_config import setup_logging
from langchain.chat_models import init_chat_model
from langgraph.graph import START, END
from pydantic import BaseModel, Field
from langchain.messages import SystemMessage, AIMessage, ToolMessage, HumanMessage
from app.tools.search import get_paper_details
from langgraph.types import Command
from langchain.tools import ToolRuntime
from langgraph.graph.ui import push_ui_message
from langgraph.prebuilt import ToolNode, tools_condition
from typing import Literal
from app.agent.ui_manager import UIManager
from app.core.schema import StepName, StepStatus
from app.agent.utils import get_paper_abstract
DO_NOT_RENDER_ID_PREFIX = "do-not-render-"


setup_logging()

logger = logging.getLogger(__name__)


def _optimize_for_search(messages: list) -> str:
    """Derive a keyword-style search query from the full conversation history."""
    class SearchQueryOutput(BaseModel):
        search_query: str = Field(
            description=(
                "A keyword-style query for semantic academic paper retrieval. "
                "5-15 words. Include core concepts, method names, domain terms, "
                "and any paper/author names mentioned in the conversation. "
                "Do NOT write a full sentence â€” use noun phrases and keywords only. "
                "Example: 'transformer self-attention mechanism NLP Vaswani 2017'"
            )
        )

    system = SystemMessage(content=(
        "You are an expert at generating academic paper search queries. "
        "Given the conversation history below, produce a keyword-style search query "
        "that captures what the user wants to find. "
        "Resolve all references (e.g. 'that paper', 'it', 'their method') using context from earlier messages."
    ))
    model = init_chat_model(model=settings.AGENT_MODEL_NAME, api_key=settings.GEMINI_API_KEY)
    structured = model.bind_tools([SearchQueryOutput], tool_choice="SearchQueryOutput")
    msg = structured.invoke([system] + messages)
    return msg.tool_calls[0]["args"]["search_query"]


def _optimize_for_qa(messages: list) -> str:
    """Derive a fully self-contained research question from the full conversation history."""
    class QAQueryOutput(BaseModel):
        qa_query: str = Field(
            description=(
                "A fully self-contained research question. "
                "All pronouns and vague references must be replaced with explicit names "
                "using context from earlier messages. "
                "The question must make complete sense without any prior context."
            )
        )

    system = SystemMessage(content=(
        "You are an expert at reformulating research questions. "
        "Given the conversation history below, rewrite the user's latest question "
        "as a fully self-contained research question. "
        "Resolve all references (e.g. 'that paper', 'it', 'their method', 'the above') "
        "using context from earlier messages."
    ))
    model = init_chat_model(model=settings.AGENT_MODEL_NAME, api_key=settings.GEMINI_API_KEY)
    structured = model.bind_tools([QAQueryOutput], tool_choice="QAQueryOutput")
    msg = structured.invoke([system] + messages)
    return msg.tool_calls[0]["args"]["qa_query"]


@tool
def find_papers(runtime: ToolRuntime) -> Command:
    """
    Find papers using the optimized query generated by the previous workflow.
    It would update the current papers list with the new papers found.
    Trust the result from the tools would find the most relevant papers.
    """

    messages = runtime.state.get("messages", [])
    papers = runtime.state.get("papers", [])

    search_query = _optimize_for_search(messages)
    logger.info(f"Finding papers for search query: {search_query[:100]}{'...' if len(search_query) > 100 else ''}")
    logger.debug(f"Current paper count: {len(papers)}")

    state = {"optimized_query": search_query, "papers": papers, "messages": [HumanMessage(content=search_query)]}
    result = paper_finder_fast_graph.invoke(state)
    papers = result["papers"]

    logger.info(f"Found {len(papers)} papers")

    # Return papers via Command - UI message will be pushed from agent node
    return Command(
        update={"papers": result["papers"],
        "messages": [ToolMessage(content=f"I found {len(result['papers'])} papers for your query.", tool_call_id=runtime.tool_call_id)]
        })

@tool
def retrieve_and_answer_question(runtime: ToolRuntime) -> str:
    """
    Answer the user question based on the current papers.
    The tool would retrieve evidence from the selected papers and answer the user question based on the evidence.
    Return:
    - The answer to the user's question
    """
    messages = runtime.state.get("messages", [])
    papers = runtime.state.get("papers", [])
    selected_ids = runtime.state.get("selected_paper_ids", [])

    user_query = _optimize_for_qa(messages)
    logger.info(f"Answering question for query: {user_query[:100]}{'...' if len(user_query) > 100 else ''}")
    logger.debug(f"Selected paper count: {len(selected_ids)}, Total papers: {len(papers)}")

    if not selected_ids:
        logger.warning("No papers selected for QA")
        return "No papers selected for QA. Please select papers first."

    # If user selected specific papers, use those for QA
    qa_state = {
        "messages": [HumanMessage(content=user_query)],
        "papers": papers,
        "selected_paper_ids": selected_ids,
        "user_query": user_query,
    }

    logger.debug("Invoking QA graph")
    result = qa_graph.invoke(qa_state)
    logger.info("QA graph completed successfully")
    return result["final_answer"]

def query_clarification(state: SupervisorState):
    # create a message at the top for the step tracking
    steps = []
    logger.debug("Query clarification node invoked")
    new_steps = []
    new_ui_tracking_message = AIMessage(id=str(uuid.uuid4()), content="")
    new_ui_tracking_id = str(uuid.uuid4())
    new_ui_state = {
        "steps": [],
        "ui_tracking_message": new_ui_tracking_message,
        "ui_tracking_id": new_ui_tracking_id
    }
    ui_manager = UIManager.from_state(new_ui_state)
    new_steps = ui_manager.update_ui(StepName.QUERY_CLARIFICATION, StepStatus.RUNNING)
    system_prompt = f"""
    You are an expert in clarifying user queries for a research assistant.
    You need to decide if the user's query is clear or it needs clarification.
    Take the previous messages into account if there is any.
    Make the decision and provide your reasoning for the decision.
    """

    class QueryIsClear(BaseModel):
        reasoning: str = Field(description="The reasoning why the user's query is clear")

    class QueryNeedsClarification(BaseModel):
        reasoning: str = Field(description="The reasoning why the user's query needs clarification")
        clarification: str = Field(description="The clarification for the user's query")

    tools = [QueryIsClear, QueryNeedsClarification]
    structured_model = supervisor_model.bind_tools(tools, tool_choice="any")
    msg = structured_model.invoke([
        SystemMessage(content=system_prompt)
    ] + state["messages"])

    response = msg.tool_calls[0]["args"]

    if "clarification" not in response:
        logger.info("Query is clear, proceeding to optimization")
        new_steps = ui_manager.update_ui(StepName.QUERY_CLARIFICATION, StepStatus.COMPLETED)
        return {
            "messages": [new_ui_tracking_message],
            "is_clear": True,
            "steps": new_steps,
            "ui_tracking_message": new_ui_tracking_message,
            "ui_tracking_id": new_ui_tracking_id
        }
    else:
        logger.info("Query needs clarification, requesting user input")
        logger.debug(f"Clarification requested: {response['clarification'][:100]}...")
        new_steps = ui_manager.update_ui(StepName.QUERY_CLARIFICATION, StepStatus.COMPLETED, response["clarification"])
        return {
            "messages": [new_ui_tracking_message, AIMessage(content=response["clarification"])],
            "is_clear": False,
            "steps": new_steps,
            "ui_tracking_message": new_ui_tracking_message,
            "ui_tracking_id": new_ui_tracking_id
        }

def should_clarify(state: SupervisorState):
    is_clear = state.get("is_clear", True)
    route = "supervisor" if is_clear else "end"
    return route

supervisor_model = init_chat_model(model=settings.AGENT_MODEL_NAME, api_key=settings.GEMINI_API_KEY)
tools = [find_papers, get_paper_details, retrieve_and_answer_question]

supervisor_prompt = """
You are a supervisor for a research assistant system.
You are provided with two tools: find_papers, and get_paper_details.
The find_papers tool is used to find papers related to the user query.
The get_paper_details tool is used to get the details of a paper currently in the paper list.
If you don't have enough papers, you should call the find_papers tool.
And then you should call the get_paper_details tool to get the details of the papers.
You can only call the find papers tool once.
"""

# Bind tools to the supervisor model
supervisor_model_with_tools = supervisor_model.bind_tools(tools)

@tool
def generate_plan(plan_choice: Literal["find_then_qa", "find_only", "qa_only"]) -> str:
    """
    Generate a plan for the research assistant to follow.
    Args: 
    - plan_choice: The plan you are going to follow.
        - "qa_only": Choose this plan if you think the current paper list is enough to answer the user's question
        - "find_then_qa": Choose this plan if user have a specific question about some papers that is not present in the current paper list
        - "find_only": Choose this plan if the user is interested in finding certain papers and does not ask any specific question about them
    Returns:
    - plan: The plan you are going to follow.
    """
    return ""
    
def supervisor_agent_node(state: SupervisorState):
    """
    Supervisor agent node that:
    1. Checks if last message is a ToolMessage from find_papers
    2. If so, extracts papers and calls push_ui_message
    3. Otherwise, invokes the model to decide next action

    Note: Messages with id starting with "do-not-render-" are hidden from UI
    but remain in history for model reasoning.
    """
    ui_manager = UIManager.from_state(state)
    logger.debug("Supervisor agent node invoked")

    messages = state["messages"]
    last_message = messages[-1] if messages else None
    # ========================================
    # POST-PROCESSING: Handle tool results
    # ========================================
    paper_list_ui_message = None
    if isinstance(last_message, ToolMessage):
        logger.info(f"Post-processing ToolMessage from: {last_message.name}")

        if last_message.name == "find_papers":
            papers = state.get("papers", [])
            logger.debug(f"Retrieved {len(papers)} papers from state")

            if papers and "found" in last_message.content.lower() and "papers" in last_message.content.lower():
                logger.info(f"Pushing UI message with {len(papers)} papers")

                try:
                    # Convert S2Paper objects to dicts if needed
                    papers_data = [p.model_dump() if hasattr(p, 'model_dump') else p for p in papers]
                    logger.debug(f"Converted {len(papers_data)} papers to dict format")

                    # Create AI message
                    paper_list_ui_message = AIMessage(
                        id=str(uuid.uuid4()),
                        content="",
                    )

                    ui_msg = push_ui_message(
                        "papers",
                        {"papers": papers_data},
                        message=paper_list_ui_message
                    )

                    logger.info(f"UI message pushed successfully: {ui_msg['id']}")

                    ui_manager.update_ui(StepName.FIND_PAPERS, StepStatus.COMPLETED, len(papers))

                except Exception as e:
                    logger.error(f"Failed to push UI message: {e}", exc_info=True)
            else:
                logger.warning("find_papers returned but no papers found or unexpected message format")

        elif last_message.name == "retrieve_and_answer_question":
            logger.info("retrieve_and_answer_question completed successfully")
            ui_manager.update_ui(StepName.RETRIEVE_AND_ANSWER_QUESTION, StepStatus.COMPLETED)

        else:
            logger.warning(f"Unknown tool call: {last_message.name}")
    
    # ========================================
    # PLAN GENERATION & EXECUTION
    # ========================================
    plan = state.get("plan_steps", [])

    # Generate plan if it doesn't exist
    if len(plan) == 0:
        logger.info("Generating new plan")
        ui_manager.update_ui(StepName.PLAN, StepStatus.RUNNING)
        # Check if user has selected specific papers
        selected_ids = state.get("selected_paper_ids", [])
        selected_paper_abstracts = ""
        if selected_ids:
            logger.info(f"User has selected {len(selected_ids)} specific papers for focused analysis")
            selected_paper_abstracts = get_paper_abstract(state.get("papers", []), selected_ids)

        plan_prompt = f"""
        You are a planner for a research assistant system.
        The research assistant system should help answer the user's question.
        The system is capable of finding papers based on relevant to the user's question and retrieve evidence from the paper to answer the user's question.
        You are provided with a tool to generate structured plan for the research assistant system to follow.
        To correctly generate the plan you are provided with the chat history, the abstract of the papers that the system have found.

        Goal:
        - Generate a plan for the research assistant system to follow using the provided tool.

        General Guidelines:
        - Focus on the chat history and the last user's message to generate the plan.
        - The paper abstracts should tell you what has been retrieved so far
        - If the user's query is about some papers that are not present in the current paper list. Choose the qa_only plan.
        - Distinguish between the find_then_qa and find_only plans
           - If the user's query is only about finding papers, for example "Find me some papers about the topic of AI?", "Can you help me find some papers about AI agent?" Choose the find_only plan.
           - If the user's query is a question about certain papers that are not present in the current paper list, Choose the find_then_qa plan. For example, "Can you explain the methodology in paper X?" "Find me the paper X and tell me what is it about?"
        
        The following is the paper abstracts that the system have found so far:
        {selected_paper_abstracts}
        Here is the chat history:
        """
        planner = supervisor_model_with_tools.bind_tools([generate_plan], tool_choice="generate_plan")
        logger.debug("Invoking planner model")
        response = planner.invoke([
            SystemMessage(content=plan_prompt),
            *state.get("messages", []),
            HumanMessage(content="Generate the plan")
        ])

        plan_choice = response.tool_calls[0]["args"]["plan_choice"]
        logger.info(f"Planner chose: {plan_choice}")
        new_steps = ui_manager.update_ui(StepName.PLAN, StepStatus.COMPLETED, plan_choice)
        if plan_choice == "qa_only":
            plan = ["retrieve_and_answer_question", "end"]
        elif plan_choice == "find_then_qa":
            plan = ["find_papers", "retrieve_and_answer_question", "end"]
        elif plan_choice == "find_only":
            plan = ["find_papers", "end"]
        else:
            logger.error(f"Invalid plan choice: {plan_choice}")
            raise ValueError(f"Invalid plan choice: {plan_choice}")

        logger.info(f"Generated plan: {plan}")

    # Execute next step in plan
    if not plan:
        logger.warning("Plan is empty, ending execution")
        return {}

    current_step = plan.pop(0)
    logger.info(f"Executing plan step: {current_step}")

    if current_step == "end":
        logger.info("Plan completed successfully")
        if last_message.name == "retrieve_and_answer_question":
            return {"plan_steps": plan, "messages": [AIMessage(content=last_message.content)]}
        elif last_message.name == "find_papers":
            if paper_list_ui_message:
                logger.info("find_papers completed successfully")
                return {"plan_steps": plan, "messages": [paper_list_ui_message]}
            else:
                logger.warning("find_papers completed but no UI message pushed")
                return {"plan_steps": plan}
        else:
            return {"plan_steps": plan}
    # Update UI to show the step is starting
    if current_step == "find_papers":
        new_steps = ui_manager.update_ui(StepName.FIND_PAPERS, StepStatus.RUNNING)
    elif current_step == "retrieve_and_answer_question":
        new_steps = ui_manager.update_ui(StepName.RETRIEVE_AND_ANSWER_QUESTION, StepStatus.RUNNING)
    else:
        new_steps = ui_manager.steps
    # Create manual tool call for the next step
    manual_tool_call = {
        "name": current_step,
        "args": {},
        "id": str(uuid.uuid4())
    }

    return {"messages": [AIMessage(content="", tool_calls=[manual_tool_call])], "plan_steps": plan, "steps": new_steps}

supervisor_tool_node = ToolNode(tools)


# Build the graph with decomposed supervisor
graph = StateGraph(SupervisorState)
graph.add_node("query_clarification", query_clarification)
graph.add_node("supervisor_agent", supervisor_agent_node)
graph.add_node("supervisor_tools", supervisor_tool_node)

graph.add_edge(START, "query_clarification")
graph.add_conditional_edges("query_clarification", should_clarify, {"supervisor": "supervisor_agent", "end": END})

# Add conditional edges for the supervisor agent loop
graph.add_conditional_edges(
    "supervisor_agent",
    tools_condition,
    {
        "tools": "supervisor_tools",
        "__end__": END
    }
)
graph.add_edge("supervisor_tools", "supervisor_agent")

graph = graph.compile().with_config({"recursion_limit": 100})