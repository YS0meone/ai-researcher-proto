import uuid
from langchain_core.tools import tool
from app.db.schema import S2Paper
from app.agent.paper_finder import paper_finder
from app.agent.paper_finder_fast import paper_finder_fast_graph
from app.agent.qa import qa_graph
from typing import Tuple
from langgraph.graph import StateGraph
from app.agent.states import State
import logging
from app.core.config import settings
from langchain.chat_models import init_chat_model
from langgraph.graph import START, END
from app.agent.utils import setup_langsmith
from pydantic import BaseModel, Field
from langchain.messages import SystemMessage, AIMessage, ToolMessage, HumanMessage
from app.tools.search import get_paper_details
from langgraph.types import Command
from langchain.tools import ToolRuntime
from langgraph.graph.ui import AnyUIMessage, ui_message_reducer, push_ui_message
from langgraph.prebuilt import ToolNode, tools_condition
from typing import Literal, List
from app.agent.utils import get_paper_info_text


setup_langsmith()
logger = logging.getLogger(__name__)

@tool
def find_papers(runtime: ToolRuntime) -> Command:
    """
    Find papers using the optimized query generated by the previous workflow.
    It would update the current papers list with the new papers found.
    Trust the result from the tools would find the most relevant papers.
    """

    user_query = runtime.state.get("optimized_query", "")
    papers = runtime.state.get("papers", [])

    state = {"optimized_query": user_query, "papers": papers, "messages": [HumanMessage(content=user_query)]}
    result = paper_finder_fast_graph.invoke(state)
    papers = result["papers"]
    
    # Return papers via Command - UI message will be pushed from agent node
    return Command(
        update={"papers": result["papers"], 
        "messages": [ToolMessage(content=f"I found {len(result['papers'])} papers for your query.", tool_call_id=runtime.tool_call_id)]
        })

@tool
def answer_question(runtime: ToolRuntime) -> str:
    """
    Answer the user question based on the current papers.
    If user has selected specific papers, focus the analysis on those papers.
    Trust the result from the tools would answer the user question based on the papers and forward the answer to the user.
    """
    user_query = runtime.state.get("optimized_query", "")
    papers = runtime.state.get("papers", [])
    selected_ids = runtime.state.get("selected_paper_ids", [])

    # If user selected specific papers, use those for QA
    qa_state = {
        "messages": [HumanMessage(content=user_query)],
        "papers": papers,
    }

    if selected_ids:
        qa_state["selected_ids"] = selected_ids
        print(f"üìå [ANSWER_QUESTION] Using {len(selected_ids)} user-selected papers for QA")

    # result = qa_graph.invoke(qa_state)
    # return result["messages"][-1].content

    # Placeholder response until QA graph is fully implemented
    context_note = f" focusing on {len(selected_ids)} selected papers" if selected_ids else ""
    return f"Analysis complete{context_note}. The papers address your question."

def query_clarification(state: State):
    system_prompt = f"""
    You are an expert in clarifying user queries for a research assistant.
    You need to decide if the user's query is clear or it needs clarification.
    Take the previous messages into account if there is any.
    Make the decision and provide your reasoning for the decision.
    """
    
    class QueryIsClear(BaseModel):
        reasoning: str = Field(description="The reasoning why the user's query is clear")

    class QueryNeedsClarification(BaseModel):
        reasoning: str = Field(description="The reasoning why the user's query needs clarification")
        clarification: str = Field(description="The clarification for the user's query")
    
    tools = [QueryIsClear, QueryNeedsClarification]
    structured_model = supervisor_model.bind_tools(tools, tool_choice="any")
    msg = structured_model.invoke([
        SystemMessage(content=system_prompt)
    ] + state["messages"])
    
    response = msg.tool_calls[0]["args"]

    if "clarification" not in response:
        return {"is_clear": True}
    else:
        return {"messages": [AIMessage(content=response["clarification"])], "is_clear": False}

def query_optimization(state: State):
    system_prompt = f"""
    You are an expert in optimizing user queries for a search agent for academic papers.
    Your goals is to rephrase the user query to be more specific and to be more likely to help the subagent find the most relevant papers and answer the user's question.
    There might be some clarification happened before this node, you should take that into account.
    If the user's query is good enough, you may repeat the user's query as the optimized query or change it slightly to be more specific.
    If the user's query is not good enough, you should optimize it.
    The optimized query should be self-contained and should not require any additional context.
    """

    class QueryOptimizationOutput(BaseModel):
        reasoning: str = Field(description="The reasoning for your optimization")
        optimized_query: str = Field(description="The optimized query for the user's query")

    tools = [QueryOptimizationOutput]
    structured_model = supervisor_model.bind_tools(tools, tool_choice="QueryOptimizationOutput")
    msg = structured_model.invoke([
        SystemMessage(content=system_prompt)
    ] + state["messages"])
    response = msg.tool_calls[0]["args"]

    message = f"The optimized query is: {response['optimized_query']}\n\nReasoning: {response['reasoning']}"
    
    return {"messages": [AIMessage(content=message)], "optimized_query": response['optimized_query']}

def should_clarify(state: State):
    is_clear = state.get("is_clear", True)
    route = "optimize" if is_clear else "end"
    return route

supervisor_model = init_chat_model(model=settings.AGENT_MODEL_NAME, api_key=settings.GEMINI_API_KEY)
tools = [find_papers, get_paper_details]

supervisor_prompt = """
You are a supervisor for a research assistant system.
You are provided with two tools: find_papers, and get_paper_details.
The find_papers tool is used to find papers related to the user query.
The get_paper_details tool is used to get the details of a paper currently in the paper list.
If you don't have enough papers, you should call the find_papers tool.
And then you should call the get_paper_details tool to get the details of the papers.
You can only call the find papers tool once.
"""

# Bind tools to the supervisor model
supervisor_model_with_tools = supervisor_model.bind_tools(tools)

@tool
def generate_plan(plan_choice: Literal["find_then_qa", "find_only", "qa_only"]) -> str:
    """
    Generate a plan for the research assistant to follow.
    Args: 
    - plan_choice: The plan you are going to follow.
        - "qa_only": Choose this plan if you think the current paper list is enough to answer the user's question
        - "find_then_qa": Choose this plan if user have a specific question about some papers that is not present in the current paper list
        - "find_only": Choose this plan if the user is interested in finding certain papers and does not ask any specific question about them
    Returns:
    - plan: The plan you are going to follow.
    """
    return ""
    
def supervisor_agent_node(state: State):
    """
    Supervisor agent node that:
    1. Checks if last message is a ToolMessage from find_papers
    2. If so, extracts papers and calls push_ui_message
    3. Otherwise, invokes the model to decide next action
    """

    messages = state["messages"]
    last_message = messages[-1] if messages else None
    
    # ========================================
    # POST-PROCESSING: Handle tool results
    # ========================================
    if isinstance(last_message, ToolMessage):
        print(f"üìù [SUPERVISOR NODE] Post-processing ToolMessage from: {last_message.name}")
        
        if last_message.name == "find_papers":
            papers = state.get("papers", [])
            
            if papers and "found" in last_message.content.lower() and "papers" in last_message.content.lower():
                print(f"üé® [SUPERVISOR NODE] Pushing UI message with {len(papers)} papers")
                
                try:
                    # Convert S2Paper objects to dicts if needed
                    papers_data = [p.model_dump() if hasattr(p, 'model_dump') else p for p in papers]
                    
                    # Create AI message
                    ai_message = AIMessage(
                        id=str(uuid.uuid4()),
                        content=f"I found {len(papers)} papers for your query.",
                    )
                    
                    # Push UI message FROM NODE (officially supported!)
                    ui_msg = push_ui_message(
                        "papers", 
                        {"papers": papers_data}, 
                        message=ai_message
                    )
                    
                    print(f"[OK] [SUPERVISOR NODE] UI message pushed successfully: {ui_msg['id']}")
                    
                    # Return AI message to state and continue to next plan step
                    return {"messages": [ai_message]}
                
                except Exception as e:
                    print(f"[ERROR] [SUPERVISOR NODE] Failed to push UI message: {e}")
                    import traceback
                    traceback.print_exc()
                    # Continue to next plan step even if UI push fails
                    return {}
            else:
                # No papers or unexpected message format
                print(f"‚ö†Ô∏è [SUPERVISOR NODE] find_papers returned but no papers found")
                return {}
                
        elif last_message.name == "answer_question":
            print("‚úÖ [SUPERVISOR NODE] answer_question completed")
            return {"messages": [AIMessage(content=last_message.content)]}
            
        else:
            print(f"‚ö†Ô∏è [SUPERVISOR NODE] Unknown tool call: {last_message.name}")
            return {}
    
    # ========================================
    # PLAN GENERATION & EXECUTION
    # ========================================
    plan = state.get("plan_steps", [])
    
    # Generate plan if it doesn't exist
    if len(plan) == 0:
        print("üìã [SUPERVISOR NODE] Generating new plan")

        # Check if user has selected specific papers
        selected_ids = state.get("selected_paper_ids", [])
        selected_context = ""
        if selected_ids:
            selected_context = f"\n\nIMPORTANT: User has selected {len(selected_ids)} specific papers for focused analysis (IDs: {', '.join(selected_ids[:3])}{'...' if len(selected_ids) > 3 else ''}). Consider these papers when planning."

        plan_prompt = f"""
        You are a supervisor for a research assistant system.
        Generate a plan using the provided tool with provided context.{selected_context}
        """
        planner = supervisor_model_with_tools.bind_tools([generate_plan], tool_choice="generate_plan")
        planner_context = f"""
        User query: {state.get("optimized_query", "")}
        Current papers: {get_paper_info_text(state.get("papers", []))}
        """
        response = planner.invoke([
            SystemMessage(content=plan_prompt),
            HumanMessage(content=planner_context)
        ])
        plan_choice = response.tool_calls[0]["args"]["plan_choice"]
        
        if plan_choice == "qa_only":
            plan = ["answer_question", "end"]
        elif plan_choice == "find_then_qa":
            plan = ["find_papers", "answer_question", "end"]
        elif plan_choice == "find_only":
            plan = ["find_papers", "end"]
        else:
            raise ValueError(f"Invalid plan choice: {plan_choice}")
        
        print(f"üìã [SUPERVISOR NODE] Generated plan: {plan}")
    
    # Execute next step in plan
    if not plan:
        print("‚ö†Ô∏è [SUPERVISOR NODE] Plan is empty, ending")
        return {}
    
    current_step = plan.pop(0)
    print(f"üéØ [SUPERVISOR NODE] Executing plan step: {current_step}")
    
    if current_step == "end":
        print("‚úÖ [SUPERVISOR NODE] Plan completed")
        return {}
    
    # Create manual tool call for the next step
    manual_tool_call = {
        "name": current_step,
        "args": {},
        "id": str(uuid.uuid4())
    }

    return {"messages": [AIMessage(content="", tool_calls=[manual_tool_call])], "plan_steps": plan}

supervisor_tool_node = ToolNode(tools)


# Build the graph with decomposed supervisor
graph = StateGraph(State)
graph.add_node("query_clarification", query_clarification)
graph.add_node("query_optimization", query_optimization)
graph.add_node("supervisor_agent", supervisor_agent_node)
graph.add_node("supervisor_tools", supervisor_tool_node)

graph.add_edge(START, "query_clarification")
graph.add_conditional_edges("query_clarification", should_clarify, {"optimize": "query_optimization", "end": END})
graph.add_edge("query_optimization", "supervisor_agent")

# Add conditional edges for the supervisor agent loop
graph.add_conditional_edges(
    "supervisor_agent",
    tools_condition,
    {
        "tools": "supervisor_tools",
        "__end__": END
    }
)
graph.add_edge("supervisor_tools", "supervisor_agent")

graph = graph.compile().with_config({"recursion_limit": 100})